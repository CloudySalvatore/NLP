# NLP
第一题：
首先将构建所有文本的词典。
然后统计数据集中最长句子的字数。
再将每个句子填充到此数据集中最大句子字数（236字）。
然后将每个字通过词典转化为相应的词向量。
通过双向LSTM模型，对每个字生成每个类别的概率，再经过CRF，确定最终类别结果。
整个训练过程是将一个句字的每一个字输入模型训练，输出每一个字的类别。



第二题：
首先对每个句子行进中文实体识别，将识别出来的实体进行标注。
然后将每个句子的每个字通过词典转化为相应的词向量。
将标注的实体与句子一起放入模型中训练。
然后通过单向LSTM模型，产生实体之间的每个类别的概率，再经过一层全连接神经网络，确定最终类别结果。



本次项目主要运用到了python深度学习库keras。
同时还参考了一些github项目

https://github.com/crownpku/Information-Extraction-Chinese/tree/master/RE_BGRU_2ATT

https://segmentfault.com/a/1190000021214937?utm_source=tag-newest


对于中文实体识别，目前的模型因为采用的小样本训练集来训练模型，所以实体识别的精度不是很高，可以从以下几个方面优化。第一，可以在转化词向量的过程中，使用一些已经训练好的转化模型，从而提高实体识别的精度。第二加大训练集的规模，增加epoch数，使模型尽可能的收敛。

对于关系提取，目前只能做到对整句的情感大方向的分类，还不能做到实体与实体之间的关系分类，还不能将标注后的实体与整句话利用模型训练。

模型最后的accuracy：0.812
        
f1：array([0.00556587,0.02884273,0.05665048,0,0,0,0.98303516])

